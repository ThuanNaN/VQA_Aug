{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import transformers\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjnwjn59\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 59,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 64,\n",
    "    'hidden_dim': 2048,\n",
    "    'projection_dim': 2048,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,\n",
    "    'text_max_len': 50,\n",
    "    'fusion_strategy': 'concat+smalllen',\n",
    "    'text_encoder_id': 'vinai/bartpho-word',\n",
    "    'img_encoder_id': 'microsoft/beit-base-patch16-224',\n",
    "    'dataset': 'ViVQA'\n",
    "}\n",
    "PROJECT_NAME = 'vivqa_paraphrase_augmentation'\n",
    "EXP_NAME = 'vivqa_baseline_bartphoword_beit'\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=EXP_NAME,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "RANDOM_SEED = config['seed']\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample: 11999\n",
      "Number of test sample: 3001\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = Path('../../datasets')\n",
    "VIVQA_GT_TRAIN_PATH = DATASET_DIR / 'ViVQA' / 'train.csv'\n",
    "VIVQA_GT_TEST_PATH = DATASET_DIR / 'ViVQA' / 'test.csv'\n",
    "VIVQA_IMG_TRAIN_DIR = DATASET_DIR / 'MS_COCO2014' / 'merge'\n",
    "\n",
    "\n",
    "def visualize_sample(question, answer, img_path):\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    plt.imshow(img_pil)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Question: {question}?. Answer: {answer}')\n",
    "    plt.show()\n",
    "\n",
    "img_lst = os.listdir(VIVQA_IMG_TRAIN_DIR)\n",
    "\n",
    "def get_data(df_path):\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "    questions = [] \n",
    "    answers = []\n",
    "    img_paths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        img_id = row['img_id']\n",
    "        #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "        img_path = VIVQA_IMG_TRAIN_DIR / f'{img_id:012}.jpg'\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "    return questions, img_paths, answers \n",
    "\n",
    "\n",
    "train_questions, train_img_paths, train_answers = get_data(VIVQA_GT_TRAIN_PATH)    \n",
    "test_questions, test_img_paths, test_answers = get_data(VIVQA_GT_TEST_PATH)    \n",
    "\n",
    "train_set_size = len(train_questions)\n",
    "test_set_size = len(test_questions)\n",
    "\n",
    "print(f'Number of train sample: {train_set_size}')\n",
    "print(f'Number of test sample: {test_set_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(text) for text in train_questions + test_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space = set(list(train_answers + test_answers))\n",
    "idx2label = {idx: label for idx, label in enumerate(answer_space)}\n",
    "label2idx = {label: idx for idx, label in enumerate(answer_space)}\n",
    "answer_space_len = len(answer_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 10:05:48 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 1024])\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def text_tone_normalize(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory_change(directory):\n",
    "    original_directory = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_directory)\n",
    "\n",
    "TEXT_MODEL_ID = config['text_encoder_id']\n",
    "VNCORENLP_PATH = Path('../models/VnCoreNLP')\n",
    "ABS_VNCORENLP_PATH = VNCORENLP_PATH.resolve()\n",
    "os.makedirs(VNCORENLP_PATH, exist_ok=True)\n",
    "\n",
    "if not (ABS_VNCORENLP_PATH / 'models').exists():\n",
    "    py_vncorenlp.download_model(save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "with temporary_directory_change(ABS_VNCORENLP_PATH):\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], \n",
    "                                        save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_ID,\n",
    "                                    device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)\n",
    "\n",
    "sentence = 'Chúng tôi là những nghiên cứu viên.' \n",
    "sentence = text_tone_normalize(sentence, dict_map)\n",
    "segmented_sentence = ' '.join(rdrsegmenter.word_segment(sentence))\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(segmented_sentence)]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = text_model(input_ids)\n",
    "    print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(text):\n",
    "    text = text_tone_normalize(text, dict_map)\n",
    "    segmented_text = rdrsegmenter.word_segment(text)\n",
    "    segmented_text = ' '.join(segmented_text)\n",
    "\n",
    "    input_ids = torch.tensor(\n",
    "        [tokenizer.encode(segmented_text,\n",
    "                          max_length=config['text_max_len'],\n",
    "                          padding='max_length', \n",
    "                          truncation=True)]).to(device)\n",
    "    attention_mask = torch.where(input_ids == 1, 0, 1)\n",
    "\n",
    "    return { \n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 1024])\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Có bao nhiêu người trong bức ảnh ?' \n",
    "phobert_outputs = text_processor(sentence)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = text_model(**phobert_outputs)\n",
    "    print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type beit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at microsoft/beit-base-patch16-224 and are newly initialized: ['embeddings.cls_token', 'embeddings.patch_embeddings.projection.bias', 'embeddings.patch_embeddings.projection.weight', 'embeddings.position_embeddings', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.10.attention.attention.value.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.11.attention.attention.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.6.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.8.layernorm_after.weight', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.9.attention.attention.value.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'layernorm.bias', 'layernorm.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "IMG_MODEL_ID = config['img_encoder_id']\n",
    "img_processor = ViTImageProcessor.from_pretrained(IMG_MODEL_ID)\n",
    "img_model = ViTModel.from_pretrained(IMG_MODEL_ID,\n",
    "                                     device_map=device)\n",
    "inputs = img_processor(images=image, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = img_model(**inputs)\n",
    "    print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# img_transform = transforms.Compose([\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "class ViVQADataset(Dataset):\n",
    "    def __init__(self, data_dir, data_mode, text_processor, img_processor, label_encoder=None, device='cpu'):\n",
    "        self.data_dir = data_dir\n",
    "        if data_mode == 'train':\n",
    "            self.data_path = data_dir / 'ViVQA' / 'train.csv'\n",
    "        else:\n",
    "            self.data_path = data_dir / 'ViVQA' / 'test.csv'\n",
    "        self.text_processor = text_processor\n",
    "        self.img_processor = img_processor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "\n",
    "        self.questions, self.img_paths, self.answers = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        df = pd.read_csv(self.data_path, index_col=0)\n",
    "        questions = [] \n",
    "        answers = []\n",
    "        img_paths = []\n",
    "        for idx, row in df.iterrows():\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            img_id = row['img_id']\n",
    "            #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "\n",
    "            img_path = self.data_dir / 'MS_COCO2014' / 'merge' / f'{img_id:012}.jpg'\n",
    "\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "            img_paths.append(img_path)\n",
    "\n",
    "\n",
    "        return questions, img_paths, answers \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questions = self.questions[idx]\n",
    "        answers = self.answers[idx]\n",
    "        img_paths = self.img_paths[idx]\n",
    "\n",
    "        img_pil = Image.open(img_paths).convert('RGB')\n",
    "        text_inputs = self.text_processor(questions)\n",
    "        \n",
    "        img_inputs = self.img_processor(images=img_pil, \n",
    "                                        return_tensors='pt')\n",
    "        label = self.label_encoder[answers]\n",
    "        \n",
    "        text_inputs = {k: v.squeeze().to(self.device) for k, v in text_inputs.items()}\n",
    "        img_inputs = {k: v.squeeze().to(self.device) for k, v in img_inputs.items()}\n",
    "        labels = torch.tensor(label, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'text_inputs': text_inputs,\n",
    "            'img_inputs': img_inputs,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VAL_BATCH_SIZE = config['val_batch_size']\n",
    "    \n",
    "train_dataset = ViVQADataset(DATASET_DIR.resolve(), 'train', \n",
    "                             text_processor=text_processor,\n",
    "                             img_processor=img_processor, \n",
    "                             label_encoder=label2idx,\n",
    "                             device=device)\n",
    "val_dataset = ViVQADataset(DATASET_DIR.resolve(), 'val', \n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           label_encoder=label2idx,\n",
    "                           device=device)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=VAL_BATCH_SIZE,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "batch['text_inputs']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 353])\n"
     ]
    }
   ],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "        #self.norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(**inputs)\n",
    "        x = x['last_hidden_state'][:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "        #x = self.norm(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, model_name, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "        #self.norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(**inputs)\n",
    "        x = x['last_hidden_state'][:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "        #x = self.norm(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, projection_dim, hidden_dim, answer_space):\n",
    "        super().__init__()\n",
    "        #self.lstm = nn.LSTM(projection_dim * 2, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(projection_dim * 2, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, answer_space)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        # x = text_f + img_f # summation\n",
    "        x = torch.cat((img_f, text_f), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "class BERTViTVQA(nn.Module):\n",
    "    def __init__(self, text_encoder, img_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.img_encoder = img_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, text_inputs, img_inputs):\n",
    "        text_f = self.text_encoder(text_inputs)\n",
    "        img_f = self.img_encoder(img_inputs)\n",
    "\n",
    "        logits = self.classifier(text_f, img_f)\n",
    "\n",
    "        return logits\n",
    "\n",
    "PROJECTION_DIM = config['projection_dim']\n",
    "HIDDEN_DIM = config['hidden_dim']\n",
    "text_encoder = BERTEncoder(model_name=TEXT_MODEL_ID,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ViTEncoder(model_name=IMG_MODEL_ID,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "\n",
    "model = BERTViTVQA(text_encoder=text_encoder,\n",
    "                   img_encoder=img_encoder,\n",
    "                   classifier=classifier).to(device)\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "question = 'Có bao nhiêu con mèo trong bức ảnh?'\n",
    "with torch.no_grad():\n",
    "    text_inputs = text_processor(question)\n",
    "    img_inputs = img_processor(images=image, return_tensors='pt').to(device)\n",
    "    \n",
    "    logits = model(text_inputs, img_inputs)\n",
    "\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 375/375 [04:23<00:00,  1.43batch/s, Batch Loss=4.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Train loss: 4.9693\tTrain acc: 0.0632\tVal loss: 3.9213\tVal acc: 0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 375/375 [04:22<00:00,  1.43batch/s, Batch Loss=3.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: Train loss: 3.8435\tTrain acc: 0.1363\tVal loss: 3.2764\tVal acc: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=3.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: Train loss: 3.3049\tTrain acc: 0.2262\tVal loss: 2.8725\tVal acc: 0.3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=2.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4: Train loss: 2.8835\tTrain acc: 0.3140\tVal loss: 2.5643\tVal acc: 0.3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5: Train loss: 2.5713\tTrain acc: 0.3858\tVal loss: 2.4076\tVal acc: 0.4253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=1.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6: Train loss: 2.2923\tTrain acc: 0.4506\tVal loss: 2.2728\tVal acc: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=2.32] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7: Train loss: 2.0508\tTrain acc: 0.5015\tVal loss: 2.2016\tVal acc: 0.4684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=1.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8: Train loss: 1.8469\tTrain acc: 0.5539\tVal loss: 2.1498\tVal acc: 0.4864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=1.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9: Train loss: 1.6567\tTrain acc: 0.5947\tVal loss: 2.1126\tVal acc: 0.4987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=1.81] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: Train loss: 1.4880\tTrain acc: 0.6327\tVal loss: 2.1319\tVal acc: 0.5151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50:  54%|█████▎    | 201/375 [02:17<02:00,  1.44batch/s, Batch Loss=1.44] wandb: Network error (ConnectionError), entering retry loop.\n",
      "Epoch 11/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11: Train loss: 1.3234\tTrain acc: 0.6745\tVal loss: 2.1581\tVal acc: 0.5199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12: Train loss: 1.1795\tTrain acc: 0.7072\tVal loss: 2.2313\tVal acc: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=1.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13: Train loss: 1.0600\tTrain acc: 0.7385\tVal loss: 2.2681\tVal acc: 0.5264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=0.814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14: Train loss: 0.9437\tTrain acc: 0.7649\tVal loss: 2.2928\tVal acc: 0.5411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15: Train loss: 0.8577\tTrain acc: 0.7864\tVal loss: 2.3516\tVal acc: 0.5263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=0.85] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16: Train loss: 0.7695\tTrain acc: 0.8060\tVal loss: 2.3601\tVal acc: 0.5343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 17: Train loss: 0.6907\tTrain acc: 0.8262\tVal loss: 2.4737\tVal acc: 0.5380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 375/375 [04:15<00:00,  1.46batch/s, Batch Loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 18: Train loss: 0.6224\tTrain acc: 0.8437\tVal loss: 2.5046\tVal acc: 0.5503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 375/375 [04:15<00:00,  1.47batch/s, Batch Loss=0.77] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 19: Train loss: 0.5642\tTrain acc: 0.8524\tVal loss: 2.5541\tVal acc: 0.5403\n",
      "Early stopping triggered after 10 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "PATIENCE = config['patience']\n",
    "WEIGHT_DECAY = config['weight_decay']\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                             lr=LR,\n",
    "                             weight_decay=WEIGHT_DECAY)\n",
    "# step_size = EPOCHS * 0.4\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                             step_size=step_size, \n",
    "#                                             gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / logits.size(0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accs.append(acc)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    eval_acc = sum(eval_accs) / len(eval_accs)\n",
    "\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          #scheduler,\n",
    "          patience=5):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        train_batch_loss_lst = []\n",
    "        train_batch_acc_lst = []\n",
    "\n",
    "        epoch_iterator = tqdm(train_loader, \n",
    "                              desc=f'Epoch {epoch + 1}/{epochs}', \n",
    "                              unit='batch')\n",
    "        model.train()\n",
    "        for batch in epoch_iterator:\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            train_batch_loss_lst.append(loss.item())\n",
    "            train_batch_acc_lst.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_iterator.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model,\n",
    "                                     val_loader,\n",
    "                                     criterion)\n",
    "\n",
    "        train_loss = sum(train_batch_loss_lst) / len(train_batch_loss_lst)\n",
    "        train_acc = sum(train_batch_acc_lst) / len(train_batch_acc_lst)\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_loss_lst.append(val_loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f}\\tTrain acc: {train_acc:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epochs_no_improve} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    return train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n",
    "\n",
    "train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst = train(model, \n",
    "                                                                 train_loader, \n",
    "                                                                 val_loader, \n",
    "                                                                 epochs=EPOCHS, \n",
    "                                                                 criterion=criterion, \n",
    "                                                                 optimizer=optimizer, \n",
    "                                                                 #scheduler=scheduler,\n",
    "                                                                 patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# model.cpu()\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
