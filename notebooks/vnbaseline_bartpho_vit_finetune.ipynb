{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import transformers\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjnwjn59\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/server-ailab/ThangDuongTeam/thangdd/VQA_Research/notebooks/wandb/run-20240625_115857-1y3htr3c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/1y3htr3c' target=\"_blank\">vivqa_baseline_bartphoword_vit</a></strong> to <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation' target=\"_blank\">https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/1y3htr3c' target=\"_blank\">https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/1y3htr3c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/1y3htr3c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc895ae1510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'seed': 59,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 64,\n",
    "    'hidden_dim': 2048,\n",
    "    'projection_dim': 2048,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,\n",
    "    'text_max_len': 50,\n",
    "    'fusion_strategy': 'concat+smalllen',\n",
    "    'text_encoder_id': 'vinai/bartpho-word',\n",
    "    'img_encoder_id': 'google/vit-base-patch16-224',\n",
    "    'dataset': 'ViVQA'\n",
    "}\n",
    "PROJECT_NAME = 'vivqa_paraphrase_augmentation'\n",
    "EXP_NAME = 'vivqa_baseline_bartphoword_vit'\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=EXP_NAME,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "RANDOM_SEED = config['seed']\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample: 11999\n",
      "Number of test sample: 3001\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = Path('../../datasets')\n",
    "VIVQA_GT_TRAIN_PATH = DATASET_DIR / 'ViVQA' / 'train.csv'\n",
    "VIVQA_GT_TEST_PATH = DATASET_DIR / 'ViVQA' / 'test.csv'\n",
    "VIVQA_IMG_TRAIN_DIR = DATASET_DIR / 'MS_COCO2014' / 'merge'\n",
    "\n",
    "\n",
    "def visualize_sample(question, answer, img_path):\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    plt.imshow(img_pil)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Question: {question}?. Answer: {answer}')\n",
    "    plt.show()\n",
    "\n",
    "img_lst = os.listdir(VIVQA_IMG_TRAIN_DIR)\n",
    "\n",
    "def get_data(df_path):\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "    questions = [] \n",
    "    answers = []\n",
    "    img_paths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        img_id = row['img_id']\n",
    "        #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "        img_path = VIVQA_IMG_TRAIN_DIR / f'{img_id:012}.jpg'\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "    return questions, img_paths, answers \n",
    "\n",
    "\n",
    "train_questions, train_img_paths, train_answers = get_data(VIVQA_GT_TRAIN_PATH)    \n",
    "test_questions, test_img_paths, test_answers = get_data(VIVQA_GT_TEST_PATH)    \n",
    "\n",
    "train_set_size = len(train_questions)\n",
    "test_set_size = len(test_questions)\n",
    "\n",
    "print(f'Number of train sample: {train_set_size}')\n",
    "print(f'Number of test sample: {test_set_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(text) for text in train_questions + test_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space = set(list(train_answers + test_answers))\n",
    "idx2label = {idx: label for idx, label in enumerate(answer_space)}\n",
    "label2idx = {label: idx for idx, label in enumerate(answer_space)}\n",
    "answer_space_len = len(answer_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 11:59:05 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 1024])\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def text_tone_normalize(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory_change(directory):\n",
    "    original_directory = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_directory)\n",
    "\n",
    "TEXT_MODEL_ID = config['text_encoder_id']\n",
    "VNCORENLP_PATH = Path('../models/VnCoreNLP')\n",
    "ABS_VNCORENLP_PATH = VNCORENLP_PATH.resolve()\n",
    "os.makedirs(VNCORENLP_PATH, exist_ok=True)\n",
    "\n",
    "if not (ABS_VNCORENLP_PATH / 'models').exists():\n",
    "    py_vncorenlp.download_model(save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "with temporary_directory_change(ABS_VNCORENLP_PATH):\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], \n",
    "                                        save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_ID,\n",
    "                                    device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)\n",
    "\n",
    "sentence = 'Chúng tôi là những nghiên cứu viên.' \n",
    "sentence = text_tone_normalize(sentence, dict_map)\n",
    "segmented_sentence = ' '.join(rdrsegmenter.word_segment(sentence))\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(segmented_sentence)]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = text_model(input_ids)\n",
    "    print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(text):\n",
    "    text = text_tone_normalize(text, dict_map)\n",
    "    segmented_text = rdrsegmenter.word_segment(text)\n",
    "    segmented_text = ' '.join(segmented_text)\n",
    "\n",
    "    input_ids = torch.tensor(\n",
    "        [tokenizer.encode(segmented_text,\n",
    "                          max_length=config['text_max_len'],\n",
    "                          padding='max_length', \n",
    "                          truncation=True)]).to(device)\n",
    "    attention_mask = torch.where(input_ids == 1, 0, 1)\n",
    "\n",
    "    return { \n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 1024])\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Có bao nhiêu người trong bức ảnh ?' \n",
    "phobert_outputs = text_processor(sentence)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = text_model(**phobert_outputs)\n",
    "    print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "IMG_MODEL_ID = config['img_encoder_id']\n",
    "img_processor = ViTImageProcessor.from_pretrained(IMG_MODEL_ID)\n",
    "img_model = ViTModel.from_pretrained(IMG_MODEL_ID,\n",
    "                                     device_map=device)\n",
    "inputs = img_processor(images=image, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = img_model(**inputs)\n",
    "    print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# img_transform = transforms.Compose([\n",
    "#     transforms.Resize((64, 64)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "class ViVQADataset(Dataset):\n",
    "    def __init__(self, data_dir, data_mode, text_processor, img_processor, label_encoder=None, device='cpu'):\n",
    "        self.data_dir = data_dir\n",
    "        if data_mode == 'train':\n",
    "            self.data_path = data_dir / 'ViVQA' / 'train.csv'\n",
    "        else:\n",
    "            self.data_path = data_dir / 'ViVQA' / 'test.csv'\n",
    "        self.text_processor = text_processor\n",
    "        self.img_processor = img_processor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "\n",
    "        self.questions, self.img_paths, self.answers = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        df = pd.read_csv(self.data_path, index_col=0)\n",
    "        questions = [] \n",
    "        answers = []\n",
    "        img_paths = []\n",
    "        for idx, row in df.iterrows():\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            img_id = row['img_id']\n",
    "            #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "\n",
    "            img_path = self.data_dir / 'MS_COCO2014' / 'merge' / f'{img_id:012}.jpg'\n",
    "\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "            img_paths.append(img_path)\n",
    "\n",
    "\n",
    "        return questions, img_paths, answers \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questions = self.questions[idx]\n",
    "        answers = self.answers[idx]\n",
    "        img_paths = self.img_paths[idx]\n",
    "\n",
    "        img_pil = Image.open(img_paths).convert('RGB')\n",
    "        text_inputs = self.text_processor(questions)\n",
    "        \n",
    "        img_inputs = self.img_processor(images=img_pil, \n",
    "                                        return_tensors='pt')\n",
    "        label = self.label_encoder[answers]\n",
    "        \n",
    "        text_inputs = {k: v.squeeze().to(self.device) for k, v in text_inputs.items()}\n",
    "        img_inputs = {k: v.squeeze().to(self.device) for k, v in img_inputs.items()}\n",
    "        labels = torch.tensor(label, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'text_inputs': text_inputs,\n",
    "            'img_inputs': img_inputs,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VAL_BATCH_SIZE = config['val_batch_size']\n",
    "    \n",
    "train_dataset = ViVQADataset(DATASET_DIR.resolve(), 'train', \n",
    "                             text_processor=text_processor,\n",
    "                             img_processor=img_processor, \n",
    "                             label_encoder=label2idx,\n",
    "                             device=device)\n",
    "val_dataset = ViVQADataset(DATASET_DIR.resolve(), 'val', \n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           label_encoder=label2idx,\n",
    "                           device=device)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=VAL_BATCH_SIZE,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "batch['text_inputs']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 353])\n"
     ]
    }
   ],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "        #self.norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(**inputs)\n",
    "        x = x['last_hidden_state'][:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "        #x = self.norm(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, model_name, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "        #self.norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(**inputs)\n",
    "        x = x['last_hidden_state'][:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "        #x = self.norm(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, projection_dim, hidden_dim, answer_space):\n",
    "        super().__init__()\n",
    "        #self.lstm = nn.LSTM(projection_dim * 2, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(projection_dim * 2, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, answer_space)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        # x = text_f + img_f # summation\n",
    "        x = torch.cat((img_f, text_f), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "class BERTViTVQA(nn.Module):\n",
    "    def __init__(self, text_encoder, img_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.img_encoder = img_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, text_inputs, img_inputs):\n",
    "        text_f = self.text_encoder(text_inputs)\n",
    "        img_f = self.img_encoder(img_inputs)\n",
    "\n",
    "        logits = self.classifier(text_f, img_f)\n",
    "\n",
    "        return logits\n",
    "\n",
    "PROJECTION_DIM = config['projection_dim']\n",
    "HIDDEN_DIM = config['hidden_dim']\n",
    "text_encoder = BERTEncoder(model_name=TEXT_MODEL_ID,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ViTEncoder(model_name=IMG_MODEL_ID,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "\n",
    "model = BERTViTVQA(text_encoder=text_encoder,\n",
    "                   img_encoder=img_encoder,\n",
    "                   classifier=classifier).to(device)\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "question = 'Có bao nhiêu con mèo trong bức ảnh?'\n",
    "with torch.no_grad():\n",
    "    text_inputs = text_processor(question)\n",
    "    img_inputs = img_processor(images=image, return_tensors='pt').to(device)\n",
    "    \n",
    "    logits = model(text_inputs, img_inputs)\n",
    "\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 375/375 [04:20<00:00,  1.44batch/s, Batch Loss=3.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Train loss: 4.5524\tTrain acc: 0.0858\tVal loss: 3.5922\tVal acc: 0.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=3.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: Train loss: 3.2740\tTrain acc: 0.2492\tVal loss: 2.7925\tVal acc: 0.3475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 375/375 [04:12<00:00,  1.48batch/s, Batch Loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: Train loss: 2.5977\tTrain acc: 0.3894\tVal loss: 2.4211\tVal acc: 0.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4: Train loss: 2.1983\tTrain acc: 0.4644\tVal loss: 2.2253\tVal acc: 0.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5: Train loss: 1.9141\tTrain acc: 0.5253\tVal loss: 2.1050\tVal acc: 0.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=1.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6: Train loss: 1.6868\tTrain acc: 0.5779\tVal loss: 2.0444\tVal acc: 0.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 375/375 [04:17<00:00,  1.46batch/s, Batch Loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7: Train loss: 1.4729\tTrain acc: 0.6296\tVal loss: 2.0131\tVal acc: 0.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 375/375 [04:17<00:00,  1.46batch/s, Batch Loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8: Train loss: 1.2695\tTrain acc: 0.6823\tVal loss: 2.0029\tVal acc: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 375/375 [04:16<00:00,  1.46batch/s, Batch Loss=1.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9: Train loss: 1.0959\tTrain acc: 0.7240\tVal loss: 2.0729\tVal acc: 0.5297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 375/375 [04:14<00:00,  1.47batch/s, Batch Loss=1.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: Train loss: 0.9384\tTrain acc: 0.7616\tVal loss: 2.0888\tVal acc: 0.5362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 375/375 [04:14<00:00,  1.48batch/s, Batch Loss=0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11: Train loss: 0.7967\tTrain acc: 0.8004\tVal loss: 2.1303\tVal acc: 0.5370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12: Train loss: 0.6890\tTrain acc: 0.8294\tVal loss: 2.1950\tVal acc: 0.5344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13: Train loss: 0.5992\tTrain acc: 0.8542\tVal loss: 2.3009\tVal acc: 0.5388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 375/375 [04:12<00:00,  1.48batch/s, Batch Loss=0.865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14: Train loss: 0.5148\tTrain acc: 0.8712\tVal loss: 2.3787\tVal acc: 0.5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 375/375 [04:12<00:00,  1.48batch/s, Batch Loss=0.601] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15: Train loss: 0.4523\tTrain acc: 0.8899\tVal loss: 2.4811\tVal acc: 0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=0.416] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16: Train loss: 0.3979\tTrain acc: 0.9003\tVal loss: 2.4690\tVal acc: 0.5412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=0.288] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 17: Train loss: 0.3435\tTrain acc: 0.9152\tVal loss: 2.5519\tVal acc: 0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 375/375 [04:13<00:00,  1.48batch/s, Batch Loss=0.34]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 18: Train loss: 0.3052\tTrain acc: 0.9230\tVal loss: 2.6593\tVal acc: 0.5339\n",
      "Early stopping triggered after 10 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "PATIENCE = config['patience']\n",
    "WEIGHT_DECAY = config['weight_decay']\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                             lr=LR,\n",
    "                             weight_decay=WEIGHT_DECAY)\n",
    "# step_size = EPOCHS * 0.4\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                             step_size=step_size, \n",
    "#                                             gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / logits.size(0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accs.append(acc)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    eval_acc = sum(eval_accs) / len(eval_accs)\n",
    "\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          #scheduler,\n",
    "          patience=5):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        train_batch_loss_lst = []\n",
    "        train_batch_acc_lst = []\n",
    "\n",
    "        epoch_iterator = tqdm(train_loader, \n",
    "                              desc=f'Epoch {epoch + 1}/{epochs}', \n",
    "                              unit='batch')\n",
    "        model.train()\n",
    "        for batch in epoch_iterator:\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            train_batch_loss_lst.append(loss.item())\n",
    "            train_batch_acc_lst.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_iterator.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model,\n",
    "                                     val_loader,\n",
    "                                     criterion)\n",
    "\n",
    "        train_loss = sum(train_batch_loss_lst) / len(train_batch_loss_lst)\n",
    "        train_acc = sum(train_batch_acc_lst) / len(train_batch_acc_lst)\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_loss_lst.append(val_loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f}\\tTrain acc: {train_acc:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epochs_no_improve} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    return train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n",
    "\n",
    "train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst = train(model, \n",
    "                                                                 train_loader, \n",
    "                                                                 val_loader, \n",
    "                                                                 epochs=EPOCHS, \n",
    "                                                                 criterion=criterion, \n",
    "                                                                 optimizer=optimizer, \n",
    "                                                                 #scheduler=scheduler,\n",
    "                                                                 patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# model.cpu()\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
