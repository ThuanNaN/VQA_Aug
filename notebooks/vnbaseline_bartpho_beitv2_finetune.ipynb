{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import transformers\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import timm\n",
    "import gc\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjnwjn59\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/server-ailab/ThangDuongTeam/thangdd/VQA_Research/notebooks/wandb/run-20240626_132823-arjidynn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/arjidynn' target=\"_blank\">vivqa_baseline_bartphoword_beitv2</a></strong> to <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation' target=\"_blank\">https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/arjidynn' target=\"_blank\">https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/arjidynn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/wjnwjn59/vivqa_paraphrase_augmentation/runs/arjidynn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7eff99d99510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'seed': 59,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 50,\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 64,\n",
    "    'hidden_dim': 2048,\n",
    "    'projection_dim': 2048,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,\n",
    "    'text_max_len': 50,\n",
    "    'fusion_strategy': 'concat+smalllen',\n",
    "    'text_encoder_id': 'vinai/bartpho-word',\n",
    "    'img_encoder_id': 'timm/beitv2_base_patch16_224.in1k_ft_in22k', # id from timm\n",
    "    'dataset': 'ViVQA'\n",
    "}\n",
    "PROJECT_NAME = 'vivqa_paraphrase_augmentation'\n",
    "EXP_NAME = 'vivqa_baseline_bartphoword_beitv2'\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=EXP_NAME,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "RANDOM_SEED = config['seed']\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample: 11999\n",
      "Number of test sample: 3001\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = Path('../../datasets')\n",
    "VIVQA_GT_TRAIN_PATH = DATASET_DIR / 'ViVQA' / 'train.csv'\n",
    "VIVQA_GT_TEST_PATH = DATASET_DIR / 'ViVQA' / 'test.csv'\n",
    "VIVQA_IMG_TRAIN_DIR = DATASET_DIR / 'MS_COCO2014' / 'merge'\n",
    "\n",
    "\n",
    "def visualize_sample(question, answer, img_path):\n",
    "    img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    plt.imshow(img_pil)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Question: {question}?. Answer: {answer}')\n",
    "    plt.show()\n",
    "\n",
    "img_lst = os.listdir(VIVQA_IMG_TRAIN_DIR)\n",
    "\n",
    "def get_data(df_path):\n",
    "    df = pd.read_csv(df_path, index_col=0)\n",
    "    questions = [] \n",
    "    answers = []\n",
    "    img_paths = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        img_id = row['img_id']\n",
    "        #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "        img_path = VIVQA_IMG_TRAIN_DIR / f'{img_id:012}.jpg'\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "    return questions, img_paths, answers \n",
    "\n",
    "\n",
    "train_questions, train_img_paths, train_answers = get_data(VIVQA_GT_TRAIN_PATH)    \n",
    "test_questions, test_img_paths, test_answers = get_data(VIVQA_GT_TEST_PATH)    \n",
    "\n",
    "train_set_size = len(train_questions)\n",
    "test_set_size = len(test_questions)\n",
    "\n",
    "print(f'Number of train sample: {train_set_size}')\n",
    "print(f'Number of test sample: {test_set_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(text) for text in train_questions + test_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_space = set(list(train_answers + test_answers))\n",
    "idx2label = {idx: label for idx, label in enumerate(answer_space)}\n",
    "label2idx = {label: idx for idx, label in enumerate(answer_space)}\n",
    "answer_space_len = len(answer_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-26 13:28:27 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server-ailab/miniconda3/envs/vqa_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def text_tone_normalize(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory_change(directory):\n",
    "    original_directory = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_directory)\n",
    "\n",
    "TEXT_MODEL_ID = config['text_encoder_id']\n",
    "VNCORENLP_PATH = Path('../models/VnCoreNLP')\n",
    "ABS_VNCORENLP_PATH = VNCORENLP_PATH.resolve()\n",
    "os.makedirs(VNCORENLP_PATH, exist_ok=True)\n",
    "\n",
    "if not (ABS_VNCORENLP_PATH / 'models').exists():\n",
    "    py_vncorenlp.download_model(save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "with temporary_directory_change(ABS_VNCORENLP_PATH):\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], \n",
    "                                        save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_ID,\n",
    "                                       device_map=device)\n",
    "\n",
    "def text_processor(text):\n",
    "    text = text_tone_normalize(text, dict_map)\n",
    "    segmented_text = rdrsegmenter.word_segment(text)\n",
    "    segmented_text = ' '.join(segmented_text)\n",
    "\n",
    "    input_ids = torch.tensor(\n",
    "        [tokenizer.encode(segmented_text,\n",
    "                          max_length=config['text_max_len'],\n",
    "                          padding='max_length', \n",
    "                          truncation=True)]).to(device)\n",
    "    attention_mask = torch.where(input_ids == 1, 0, 1)\n",
    "\n",
    "    return { \n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "# sentence = 'Có bao nhiêu người trong bức ảnh ?' \n",
    "# phobert_outputs = text_processor(sentence)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = text_model(**phobert_outputs)\n",
    "#     print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "IMG_MODEL_ID = config['img_encoder_id']\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "img_model = timm.create_model(\n",
    "    IMG_MODEL_ID,\n",
    "    pretrained=True,\n",
    "    num_classes=0 # remove classifier nn.Linear\n",
    ").to(device)\n",
    "data_config = timm.data.resolve_model_data_config(img_model)\n",
    "img_processor = timm.data.create_transform(**data_config, \n",
    "                                           is_training=False)\n",
    "\n",
    "# output = img_model(img_processor(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n",
    "# or equivalently (without needing to set num_classes=0)\n",
    "\n",
    "\n",
    "# img_model = img_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = img_model.forward_features(img_processor(image).to(device).unsqueeze(0))\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViVQADataset(Dataset):\n",
    "    def __init__(self, data_dir, data_mode, text_processor, img_processor, label_encoder=None, device='cpu'):\n",
    "        self.data_dir = data_dir\n",
    "        if data_mode == 'train':\n",
    "            self.data_path = data_dir / 'ViVQA' / 'train.csv'\n",
    "        else:\n",
    "            self.data_path = data_dir / 'ViVQA' / 'test.csv'\n",
    "        self.text_processor = text_processor\n",
    "        self.img_processor = img_processor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "\n",
    "        self.questions, self.img_paths, self.answers = self.get_data()\n",
    "\n",
    "    def get_data(self):\n",
    "        df = pd.read_csv(self.data_path, index_col=0)\n",
    "        questions = [] \n",
    "        answers = []\n",
    "        img_paths = []\n",
    "        for idx, row in df.iterrows():\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            img_id = row['img_id']\n",
    "            #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "\n",
    "            img_path = self.data_dir / 'MS_COCO2014' / 'merge' / f'{img_id:012}.jpg'\n",
    "\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "            img_paths.append(img_path)\n",
    "\n",
    "\n",
    "        return questions, img_paths, answers \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questions = self.questions[idx]\n",
    "        answers = self.answers[idx]\n",
    "        img_paths = self.img_paths[idx]\n",
    "\n",
    "        img_pil = Image.open(img_paths).convert('RGB')\n",
    "        text_inputs = self.text_processor(questions)\n",
    "        \n",
    "        img_inputs = self.img_processor(img_pil).to(device)\n",
    "        label = self.label_encoder[answers]\n",
    "        \n",
    "        text_inputs = {k: v.squeeze().to(self.device) for k, v in text_inputs.items()}\n",
    "        #img_inputs = {k: v.squeeze().to(self.device) for k, v in img_inputs.items()}\n",
    "        labels = torch.tensor(label, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        return {\n",
    "            'text_inputs': text_inputs,\n",
    "            'img_inputs': img_inputs,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VAL_BATCH_SIZE = config['val_batch_size']\n",
    "    \n",
    "train_dataset = ViVQADataset(DATASET_DIR.resolve(), 'train', \n",
    "                             text_processor=text_processor,\n",
    "                             img_processor=img_processor, \n",
    "                             label_encoder=label2idx,\n",
    "                             device=device)\n",
    "val_dataset = ViVQADataset(DATASET_DIR.resolve(), 'val', \n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           label_encoder=label2idx,\n",
    "                           device=device)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=VAL_BATCH_SIZE,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "batch['text_inputs']['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 353])\n"
     ]
    }
   ],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_model, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = text_model\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model(**inputs)\n",
    "        x = x['last_hidden_state'][:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, img_model, projection_dim):\n",
    "        super().__init__()\n",
    "        for param in img_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.model = img_model\n",
    "        self.linear = nn.Linear(self.model.embed_dim, projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model.forward_features(inputs)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, projection_dim, hidden_dim, answer_space):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(projection_dim * 2, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, answer_space)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        x = torch.cat((img_f, text_f), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "class ViVQAModel(nn.Module):\n",
    "    def __init__(self, text_encoder, img_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.img_encoder = img_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, text_inputs, img_inputs):\n",
    "        text_f = self.text_encoder(text_inputs)\n",
    "        img_f = self.img_encoder(img_inputs)\n",
    "\n",
    "        logits = self.classifier(text_f, img_f)\n",
    "\n",
    "        return logits\n",
    "\n",
    "PROJECTION_DIM = config['projection_dim']\n",
    "HIDDEN_DIM = config['hidden_dim']\n",
    "text_encoder = TextEncoder(text_model=text_model,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ImageEncoder(img_model=img_model,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "\n",
    "model = ViVQAModel(text_encoder=text_encoder,\n",
    "                   img_encoder=img_encoder,\n",
    "                   classifier=classifier).to(device)\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "question = 'Có bao nhiêu con mèo trong bức ảnh?'\n",
    "with torch.no_grad():\n",
    "    text_inputs = text_processor(question)\n",
    "    img_inputs = img_processor(image).to(device).unsqueeze(0)\n",
    "    \n",
    "    logits = model(text_inputs, img_inputs)\n",
    "\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 375/375 [04:32<00:00,  1.38batch/s, Batch Loss=3.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Train loss: 4.2868\tTrain acc: 0.1694\tVal loss: 2.7692\tVal acc: 0.3893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 375/375 [04:19<00:00,  1.45batch/s, Batch Loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2: Train loss: 2.6299\tTrain acc: 0.4165\tVal loss: 2.2255\tVal acc: 0.4792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=2.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3: Train loss: 2.1360\tTrain acc: 0.4895\tVal loss: 1.9968\tVal acc: 0.5243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=1.79] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4: Train loss: 1.8139\tTrain acc: 0.5578\tVal loss: 1.8641\tVal acc: 0.5582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 375/375 [04:19<00:00,  1.45batch/s, Batch Loss=0.951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5: Train loss: 1.5279\tTrain acc: 0.6145\tVal loss: 1.7664\tVal acc: 0.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 375/375 [04:34<00:00,  1.37batch/s, Batch Loss=1.08] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6: Train loss: 1.2910\tTrain acc: 0.6726\tVal loss: 1.7584\tVal acc: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7: Train loss: 1.0801\tTrain acc: 0.7189\tVal loss: 1.7495\tVal acc: 0.5970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=1.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8: Train loss: 0.9183\tTrain acc: 0.7618\tVal loss: 1.8139\tVal acc: 0.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.8]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9: Train loss: 0.7828\tTrain acc: 0.7945\tVal loss: 1.7886\tVal acc: 0.6082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10: Train loss: 0.6530\tTrain acc: 0.8275\tVal loss: 1.8694\tVal acc: 0.6109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11: Train loss: 0.5607\tTrain acc: 0.8527\tVal loss: 1.9117\tVal acc: 0.6026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.303] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12: Train loss: 0.4740\tTrain acc: 0.8728\tVal loss: 1.9723\tVal acc: 0.6043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.674] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13: Train loss: 0.4181\tTrain acc: 0.8863\tVal loss: 2.0308\tVal acc: 0.6059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.386] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14: Train loss: 0.3749\tTrain acc: 0.8994\tVal loss: 2.0962\tVal acc: 0.6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15: Train loss: 0.3179\tTrain acc: 0.9136\tVal loss: 2.1426\tVal acc: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16: Train loss: 0.2860\tTrain acc: 0.9183\tVal loss: 2.1321\tVal acc: 0.6062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 375/375 [04:18<00:00,  1.45batch/s, Batch Loss=0.266] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 17: Train loss: 0.2633\tTrain acc: 0.9272\tVal loss: 2.2027\tVal acc: 0.6097\n",
      "Early stopping triggered after 10 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "PATIENCE = config['patience']\n",
    "WEIGHT_DECAY = config['weight_decay']\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                             lr=LR,\n",
    "                             weight_decay=WEIGHT_DECAY)\n",
    "# step_size = EPOCHS * 0.4\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                             step_size=step_size, \n",
    "#                                             gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / logits.size(0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accs.append(acc)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    eval_acc = sum(eval_accs) / len(eval_accs)\n",
    "\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          #scheduler,\n",
    "          patience=5):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        train_batch_loss_lst = []\n",
    "        train_batch_acc_lst = []\n",
    "\n",
    "        epoch_iterator = tqdm(train_loader, \n",
    "                              desc=f'Epoch {epoch + 1}/{epochs}', \n",
    "                              unit='batch')\n",
    "        model.train()\n",
    "        for batch in epoch_iterator:\n",
    "            text_inputs = batch.pop('text_inputs')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            train_batch_loss_lst.append(loss.item())\n",
    "            train_batch_acc_lst.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_iterator.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model,\n",
    "                                     val_loader,\n",
    "                                     criterion)\n",
    "\n",
    "        train_loss = sum(train_batch_loss_lst) / len(train_batch_loss_lst)\n",
    "        train_acc = sum(train_batch_acc_lst) / len(train_batch_acc_lst)\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_loss_lst.append(val_loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f}\\tTrain acc: {train_acc:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epochs_no_improve} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    return train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n",
    "\n",
    "train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst = train(model, \n",
    "                                                                 train_loader, \n",
    "                                                                 val_loader, \n",
    "                                                                 epochs=EPOCHS, \n",
    "                                                                 criterion=criterion, \n",
    "                                                                 optimizer=optimizer, \n",
    "                                                                 #scheduler=scheduler,\n",
    "                                                                 patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
