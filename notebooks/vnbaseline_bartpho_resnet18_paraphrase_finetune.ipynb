{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import transformers\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import timm\n",
    "import time\n",
    "import gc\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjnwjn59\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 59,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 64,\n",
    "    'hidden_dim': 2048,\n",
    "    'projection_dim': 2048,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 5,\n",
    "    'text_max_len': 50,\n",
    "    'fusion_strategy': 'concat+smalllen',\n",
    "    'text_encoder_id': 'vinai/bartpho-word',\n",
    "    'img_encoder_id': 'timm/resnet18.a1_in1k', # id from timm\n",
    "    'paraphraser_id': 'chieunq/vietnamese-sentence-paraphase',\n",
    "    'num_paraphrase': 0,\n",
    "    'paraphrase_thresh': 0,\n",
    "    'val_set_ratio': 0.1,\n",
    "    'dataset': 'ViVQA'\n",
    "}\n",
    "PROJECT_NAME = 'vivqa_paraphrase_augmentation'\n",
    "EXP_NAME = f'phase3_vivqa_bartpho_resnet18_paran{config[\"num_paraphrase\"]}_random_{config[\"paraphrase_thresh\"]}'\n",
    "# wandb.init(\n",
    "#     project=PROJECT_NAME,\n",
    "#     name=EXP_NAME,\n",
    "#     config=config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED']= str(random_seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    os.environ['CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "RANDOM_SEED = config['seed']\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_DIR = Path('../../../datasets')\n",
    "# VIVQA_GT_TRAIN_PATH = DATASET_DIR / 'ViVQA' / 'train.csv'\n",
    "# VIVQA_GT_TEST_PATH = DATASET_DIR / 'ViVQA' / 'test.csv'\n",
    "# VIVQA_IMG_TRAIN_DIR = DATASET_DIR / 'MS_COCO2014' / 'merge'\n",
    "\n",
    "\n",
    "# def visualize_sample(question, answer, img_path):\n",
    "#     img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "#     plt.imshow(img_pil)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f'Question: {question}?. Answer: {answer}')\n",
    "#     plt.show()\n",
    "\n",
    "# img_lst = os.listdir(VIVQA_IMG_TRAIN_DIR)\n",
    "\n",
    "# def get_data(df_path):\n",
    "#     df = pd.read_csv(df_path, index_col=0)\n",
    "#     questions = [] \n",
    "#     answers = []\n",
    "#     img_paths = []\n",
    "#     for idx, row in df.iterrows():\n",
    "#         question = row['question']\n",
    "#         answer = row['answer']\n",
    "#         img_id = row['img_id']\n",
    "#         #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "#         img_path = VIVQA_IMG_TRAIN_DIR / f'{img_id:012}.jpg'\n",
    "\n",
    "#         questions.append(question)\n",
    "#         answers.append(answer)\n",
    "#         img_paths.append(img_path)\n",
    "\n",
    "#     return questions, img_paths, answers \n",
    "\n",
    "\n",
    "# train_questions, train_img_paths, train_answers = get_data(VIVQA_GT_TRAIN_PATH)    \n",
    "# test_questions, test_img_paths, test_answers = get_data(VIVQA_GT_TEST_PATH)    \n",
    "\n",
    "# train_set_size = len(train_questions)\n",
    "# test_set_size = len(test_questions)\n",
    "\n",
    "# print(f'Number of train sample: {train_set_size}')\n",
    "# print(f'Number of test sample: {test_set_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample: 10799\n",
      "Number of val sample: 1200\n",
      "Number of test sample: 3001\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = Path('../../../datasets')\n",
    "VIVQA_GT_TRAIN_PATH = DATASET_DIR / 'ViVQA' / 'vivqa_train_data.csv'\n",
    "VIVQA_GT_TEST_PATH = DATASET_DIR / 'ViVQA' / 'vivqa_test_data.csv'\n",
    "\n",
    "def get_data(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    questions = [] \n",
    "    answers = []\n",
    "    img_pils = []\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row['Question']\n",
    "        answer = row['Answer']\n",
    "        #question_type = row['type'] # 0: object, 1: color, 2: how many, 3: where\n",
    "        img_base64 = base64.b64decode(row['Image'])\n",
    "        img_pil = Image.open(io.BytesIO(img_base64)).convert('RGB')\n",
    "    \n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "        img_pils.append(img_pil)\n",
    "\n",
    "    return questions, img_pils, answers \n",
    "\n",
    "\n",
    "test_questions, test_img_pils, test_answers = get_data(VIVQA_GT_TEST_PATH)    \n",
    "train_questions, train_img_pils, train_answers = get_data(VIVQA_GT_TRAIN_PATH)\n",
    "\n",
    "answer_space = set(list(train_answers + test_answers))\n",
    "idx2label = {idx: label for idx, label in enumerate(answer_space)}\n",
    "label2idx = {label: idx for idx, label in enumerate(answer_space)}\n",
    "answer_space_len = len(answer_space)\n",
    "\n",
    "train_questions, val_questions, train_img_pils, val_img_pils, train_answers, val_answers = train_test_split(train_questions, \n",
    "                                                                                                            train_img_pils, \n",
    "                                                                                                            train_answers,\n",
    "                                                                                                            test_size=config['val_set_ratio'],\n",
    "                                                                                                            shuffle=True,\n",
    "                                                                                                            random_state=RANDOM_SEED)\n",
    "\n",
    "train_set_size = len(train_questions)\n",
    "val_set_size = len(val_questions)\n",
    "test_set_size = len(test_questions)\n",
    "\n",
    "print(f'Number of train sample: {train_set_size}')\n",
    "print(f'Number of val sample: {val_set_size}')\n",
    "print(f'Number of test sample: {test_set_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(text) for text in train_questions + test_questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-09 02:40:16 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def text_tone_normalize(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory_change(directory):\n",
    "    original_directory = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_directory)\n",
    "\n",
    "TEXT_MODEL_ID = config['text_encoder_id']\n",
    "VNCORENLP_PATH = Path('../models/VnCoreNLP')\n",
    "ABS_VNCORENLP_PATH = VNCORENLP_PATH.resolve()\n",
    "os.makedirs(VNCORENLP_PATH, exist_ok=True)\n",
    "\n",
    "if not (ABS_VNCORENLP_PATH / 'models').exists():\n",
    "    py_vncorenlp.download_model(save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "with temporary_directory_change(ABS_VNCORENLP_PATH):\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], \n",
    "                                        save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text_encoder_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_ID,\n",
    "                                       device_map=device)\n",
    "\n",
    "def text_processor(text):\n",
    "    text = text_tone_normalize(text, dict_map)\n",
    "    segmented_text = rdrsegmenter.word_segment(text)\n",
    "    segmented_text = ' '.join(segmented_text)\n",
    "\n",
    "    input_ids = text_encoder_tokenizer(segmented_text,\n",
    "                                       max_length=config['text_max_len'],\n",
    "                                       padding='max_length', \n",
    "                                       truncation=True,\n",
    "                                       return_token_type_ids=False,\n",
    "                                       return_tensors='pt').to(device)\n",
    "    \n",
    "    input_ids = {k: v.squeeze() for k, v in input_ids.items()}\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "# def text_processor(texts):\n",
    "#     texts = [text_tone_normalize(text, dict_map) for text in texts]\n",
    "#     segmented_texts = [' '.join(rdrsegmenter.word_segment(text)) for text in texts]\n",
    "\n",
    "#     input_ids = text_encoder_tokenizer(segmented_texts,\n",
    "#                                        max_length=config['text_max_len'],\n",
    "#                                        padding='max_length', \n",
    "#                                        truncation=True,\n",
    "#                                        return_token_type_ids=False,\n",
    "#                                        return_tensors='pt').to(device)\n",
    " \n",
    "#     return input_ids\n",
    "\n",
    "# sentence = 'Có bao nhiêu người trong bức ảnh ?' \n",
    "# phobert_outputs = text_processor(sentence)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = text_model(**phobert_outputs)\n",
    "#     print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw )\n",
    "\n",
    "IMG_MODEL_ID = config['img_encoder_id']\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "img_model = timm.create_model(\n",
    "    IMG_MODEL_ID,\n",
    "    pretrained=True,\n",
    "    num_classes=0 # remove classifier nn.Linear\n",
    ").to(device)\n",
    "data_config = timm.data.resolve_model_data_config(img_model)\n",
    "img_processor = timm.data.create_transform(**data_config, \n",
    "                                           is_training=False)\n",
    "\n",
    "# output = img_model(img_processor(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n",
    "# or equivalently (without needing to set num_classes=0)\n",
    "\n",
    "\n",
    "# img_model = img_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = img_model.forward_features(img_processor(image).to(device).unsqueeze(0))\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "PARAPHRASER_ID = config['paraphraser_id']\n",
    "paraphraser_tokenizer = MT5Tokenizer.from_pretrained(PARAPHRASER_ID)\n",
    "paraphraser_model = MT5ForConditionalGeneration.from_pretrained(PARAPHRASER_ID).to(device)\n",
    "\n",
    "def paraphraser(text, num_return_sequences=3):\n",
    "    inputs = paraphraser_tokenizer(text, \n",
    "                                   padding='longest', \n",
    "                                   max_length=64, \n",
    "                                   return_tensors='pt',\n",
    "                                   return_token_type_ids=False).to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = paraphraser_model.generate(input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            max_length=64,\n",
    "                            num_beams=5,\n",
    "                            early_stopping=True,\n",
    "                            no_repeat_ngram_size=1,\n",
    "                            num_return_sequences=num_return_sequences)\n",
    "    \n",
    "    paraphrase_lst = []\n",
    "    for beam_output in output:\n",
    "        paraphrase_lst.append(paraphraser_tokenizer.decode(beam_output, skip_special_tokens=True))\n",
    "    return paraphrase_lst\n",
    "\n",
    "# for text in train_questions[:30]:\n",
    "#     print(f'Input: {text}')\n",
    "#     print(f'Output: {paraphase(text)}')\n",
    "#     print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ViVQADataset(Dataset):\n",
    "    def __init__(self, questions, img_pils, answers, \n",
    "                 data_mode, text_processor, img_processor, \n",
    "                 paraphraser, label_encoder=None):\n",
    "        self.questions = questions\n",
    "        self.img_pils = img_pils\n",
    "        self.answers = answers\n",
    "        self.data_mode = data_mode\n",
    "        self.text_processor = text_processor\n",
    "        self.img_processor = img_processor\n",
    "        self.paraphraser = paraphraser\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questions = self.questions[idx]\n",
    "        answers = self.answers[idx]\n",
    "        img_pils = self.img_pils[idx]\n",
    "\n",
    "        question_inputs = self.text_processor(questions)\n",
    "        #paraphrase_inputs_dict = {f'paraphrase_text_{idx+1}':  self.text_processor(text) for idx, text in enumerate(paraphrase_questions)}\n",
    "        img_inputs = self.img_processor(img_pils).to(device)\n",
    "        label = self.label_encoder[answers]\n",
    "        \n",
    "        #img_inputs = {k: v.squeeze().to(self.device) for k, v in img_inputs.items()}\n",
    "        labels = torch.tensor(label, dtype=torch.long).to(device)\n",
    "\n",
    "        text_inputs_lst = [question_inputs]\n",
    "\n",
    "        r = random.random()\n",
    "        if self.data_mode == 'train' and config['num_paraphrase'] > 0:\n",
    "            paraphrase_questions = self.paraphraser(questions, config['num_paraphrase'])\n",
    "            paraphrase_inputs_lst = [self.text_processor(text) for text in paraphrase_questions]\n",
    "\n",
    "            if r < config['paraphrase_thresh']:\n",
    "                is_fuse_para_t = torch.ones(text_model.config.hidden_size).to(device)\n",
    "            else: \n",
    "                is_fuse_para_t = torch.zeros(text_model.config.hidden_size).to(device)\n",
    "\n",
    "            text_inputs_lst += paraphrase_inputs_lst + [is_fuse_para_t]\n",
    "        \n",
    "        data_outputs = {\n",
    "            'text_inputs_lst': text_inputs_lst,\n",
    "            'img_inputs': img_inputs,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        return data_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VAL_BATCH_SIZE = config['val_batch_size']\n",
    "    \n",
    "train_dataset = ViVQADataset(questions=train_questions,\n",
    "                             img_pils=train_img_pils,\n",
    "                             answers=train_answers,\n",
    "                             data_mode='train',\n",
    "                             text_processor=text_processor,\n",
    "                             img_processor=img_processor, \n",
    "                             paraphraser=paraphraser,\n",
    "                             label_encoder=label2idx)\n",
    "val_dataset = ViVQADataset(questions=val_questions,\n",
    "                           img_pils=val_img_pils,\n",
    "                           answers=val_answers,\n",
    "                           data_mode='val',\n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           paraphraser=paraphraser,\n",
    "                           label_encoder=label2idx)\n",
    "test_dataset = ViVQADataset(questions=test_questions,\n",
    "                           img_pils=test_img_pils,\n",
    "                           answers=test_answers,\n",
    "                           data_mode='test',\n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           paraphraser=paraphraser,\n",
    "                           label_encoder=label2idx)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(RANDOM_SEED)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          worker_init_fn=seed_worker,\n",
    "                          generator=g,)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=VAL_BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=VAL_BATCH_SIZE,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_model, projection_dim):\n",
    "        super().__init__()\n",
    "        for param in text_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.model = text_model\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, text_inputs_lst):\n",
    "        if self.training and config['num_paraphrase'] > 0:\n",
    "            embed_lst = []\n",
    "            for text_inputs in text_inputs_lst[:-1]:\n",
    "                x = self.model(**text_inputs)\n",
    "                x = x['last_hidden_state'][:, 0, :]\n",
    "                embed_lst.append(x)\n",
    "        \n",
    "            para_features_t = torch.stack(embed_lst[1:], dim=1)\n",
    "            x = torch.sum(para_features_t, dim=1)\n",
    "            x *= text_inputs_lst[-1]\n",
    "            x = x + embed_lst[0]\n",
    "\n",
    "        else:\n",
    "            text_inputs = text_inputs_lst[0]\n",
    "            x = self.model(**text_inputs)\n",
    "            x = x['last_hidden_state'][:, 0, :]\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, img_model, projection_dim):\n",
    "        super().__init__()\n",
    "        for param in img_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.model = img_model\n",
    "        self.linear = nn.Linear(self.model.num_features * 7 * 7, projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model.forward_features(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, projection_dim, hidden_dim, answer_space):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(projection_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.classifier = nn.Linear(hidden_dim, answer_space)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        x = torch.cat((img_f, text_f), 1)\n",
    "        x = self.fc(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "class ViVQAModel(nn.Module):\n",
    "    def __init__(self, text_encoder, img_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.img_encoder = img_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, text_inputs, img_inputs):\n",
    "        text_f = self.text_encoder(text_inputs)\n",
    "        img_f = self.img_encoder(img_inputs)\n",
    "\n",
    "        logits = self.classifier(text_f, img_f)\n",
    "\n",
    "        return logits\n",
    "\n",
    "PROJECTION_DIM = config['projection_dim']\n",
    "HIDDEN_DIM = config['hidden_dim']\n",
    "\n",
    "text_encoder = TextEncoder(text_model=text_model,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ImageEncoder(img_model=img_model,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "model = ViVQAModel(text_encoder=text_encoder,\n",
    "                   img_encoder=img_encoder,\n",
    "                   classifier=classifier)\n",
    "# model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "# question = 'Có bao nhiêu con mèo trong bức ảnh?'\n",
    "# batch = next(iter(train_loader))\n",
    "# # batch['text_inputs_lst'][0]['input_ids'].shape\n",
    "# with torch.no_grad():\n",
    "#     text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "#     img_inputs = batch.pop('img_inputs')\n",
    "#     labels = batch.pop('labels')\n",
    "\n",
    "#     logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "#     print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/338 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 338/338 [02:55<00:00,  1.92batch/s, Batch Loss=5.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Train loss: 5.8652\tTrain acc: 0.0038\tVal loss: 5.8556\tVal acc: 0.0066\n"
     ]
    }
   ],
   "source": [
    "def save_model(save_path, model):\n",
    "    try:\n",
    "        state_dict = model.module.state_dict()\n",
    "    except AttributeError:\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "def free_vram(model, optimizer):\n",
    "    del model\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "PATIENCE = config['patience']\n",
    "WEIGHT_DECAY = config['weight_decay']\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "SAVE_FOLDER = f'weights_{timestamp}'\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "SAVE_BEST_PATH = f'./{SAVE_FOLDER}/{EXP_NAME}_best.pt'\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                             lr=LR,\n",
    "                             weight_decay=WEIGHT_DECAY,)\n",
    "# step_size = EPOCHS * 0.4\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                             step_size=step_size, \n",
    "#                                             gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / logits.size(0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accs.append(acc)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    eval_acc = sum(eval_accs) / len(eval_accs)\n",
    "\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          #scheduler,\n",
    "          patience=5,\n",
    "          save_best_path='./weights/best.pt'):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        train_batch_loss_lst = []\n",
    "        train_batch_acc_lst = []\n",
    "\n",
    "        epoch_iterator = tqdm(train_loader, \n",
    "                              desc=f'Epoch {epoch + 1}/{epochs}', \n",
    "                              unit='batch')\n",
    "        model.train()\n",
    "        for batch in epoch_iterator:\n",
    "            text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            train_batch_loss_lst.append(loss.item())\n",
    "            train_batch_acc_lst.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_iterator.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        val_loss, val_acc = evaluate(model,\n",
    "                                     val_loader,\n",
    "                                     criterion)\n",
    "\n",
    "        train_loss = sum(train_batch_loss_lst) / len(train_batch_loss_lst)\n",
    "        train_acc = sum(train_batch_acc_lst) / len(train_batch_acc_lst)\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_loss_lst.append(val_loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "\n",
    "        # wandb.log({\n",
    "        #     'epoch': epoch + 1,\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'train_acc': train_acc,\n",
    "        #     'val_loss': val_loss,\n",
    "        #     'val_acc': val_acc\n",
    "        # })\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f}\\tTrain acc: {train_acc:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_model(save_best_path, model)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epochs_no_improve} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    return train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n",
    "\n",
    "train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst = train(model, \n",
    "                                                                 train_loader, \n",
    "                                                                 val_loader, \n",
    "                                                                 epochs=EPOCHS, \n",
    "                                                                 criterion=criterion, \n",
    "                                                                 optimizer=optimizer, \n",
    "                                                                 #scheduler=scheduler,\n",
    "                                                                 patience=PATIENCE,\n",
    "                                                                 save_best_path=SAVE_BEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.8556\tVal acc: 0.0066\n",
      "Test loss: 5.8553\tTest acc: 0.0053\n"
     ]
    }
   ],
   "source": [
    "free_vram(model, optimizer)\n",
    "\n",
    "text_encoder = TextEncoder(text_model=text_model,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ImageEncoder(img_model=img_model,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "best_model = ViVQAModel(text_encoder=text_encoder,\n",
    "                        img_encoder=img_encoder,\n",
    "                        classifier=classifier).to(device)\n",
    "\n",
    "best_model.load_state_dict(torch.load(SAVE_BEST_PATH))\n",
    "\n",
    "\n",
    "val_loss, val_acc = evaluate(model=best_model,\n",
    "                               val_loader=val_loader,\n",
    "                               criterion=criterion)\n",
    "val_loss, val_acc = round(val_loss, 4), round(val_acc, 4)\n",
    "\n",
    "test_loss, test_acc = evaluate(model=best_model,\n",
    "                               val_loader=test_loader,\n",
    "                               criterion=criterion)\n",
    "test_loss, test_acc = round(test_loss, 4), round(test_acc, 4)\n",
    "\n",
    "print(f'Val loss: {val_loss}\\tVal acc: {val_acc}')\n",
    "print(f'Test loss: {test_loss}\\tTest acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_table = wandb.Table(\n",
    "#     columns=list(config.keys())+['val_loss', 'val_acc', 'test_loss', 'test_acc'], \n",
    "#     data=[list(config.values()) + [val_loss, val_acc, test_loss, test_acc]]\n",
    "# )\n",
    "# wandb.log({\"Exp_table\": exp_table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
