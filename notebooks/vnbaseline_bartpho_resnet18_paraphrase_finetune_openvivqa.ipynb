{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import transformers\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import random\n",
    "import timm\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjnwjn59\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 59,\n",
    "    'learning_rate': 1e-5,\n",
    "    'epochs': 30,\n",
    "    'train_batch_size': 8,\n",
    "    'val_batch_size': 64,\n",
    "    'hidden_dim': 2048,\n",
    "    'projection_dim': 2048,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 5,\n",
    "    'text_max_len': 50,\n",
    "    'fusion_strategy': 'concat+smalllen',\n",
    "    'text_encoder_id': 'vinai/bartpho-word',\n",
    "    'img_encoder_id': 'timm/resnet18.a1_in1k', # id from timm\n",
    "    'paraphraser_id': 'chieunq/vietnamese-sentence-paraphase',\n",
    "    'num_paraphrase': 2,\n",
    "    'paraphrase_thresh': 0.9\n",
    "}\n",
    "PROJECT_NAME = 'openvivqa_paraphrase_augmentation'\n",
    "EXP_NAME = f'bartpho_resnet18_paran{config[\"num_paraphrase\"]}_random_{config[\"paraphrase_thresh\"]}_summation'\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    name=EXP_NAME,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "RANDOM_SEED = config['seed']\n",
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sample: 30833\n",
      "Number of val sample: 3545\n",
      "Number of test sample: 14035\n",
      "Question max length: 133\n",
      "Answer space length: 27896\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = Path('../../../datasets/OpenViVQA')\n",
    "\n",
    "def get_data(root_dir, split):\n",
    "    if split == 'train':\n",
    "        json_path = root_dir / 'vlsp2023_train_data.json'\n",
    "        img_dir = root_dir / 'training-images'\n",
    "    elif split == 'val':\n",
    "        json_path = root_dir / 'vlsp2023_dev_data.json'\n",
    "        img_dir = root_dir / 'dev-images'\n",
    "    else:\n",
    "        json_path = root_dir / 'vlsp2023_test_data.json'\n",
    "        img_dir = root_dir / 'test-images'\n",
    "        \n",
    "    with open(json_path, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    images_mapping = data_dict['images']    \n",
    "    annotations = data_dict['annotations']\n",
    "    \n",
    "    sample_ids = []\n",
    "    questions = [] \n",
    "    answers = []\n",
    "    img_paths = []\n",
    "    \n",
    "    for sample_id, sample in annotations.items():\n",
    "        img_id = str(sample['image_id'])\n",
    "        question = sample['question']\n",
    "\n",
    "        img_name = images_mapping[img_id]\n",
    "        img_path = img_dir / img_name\n",
    "\n",
    "        if split != 'test':\n",
    "            answer = sample['answer']\n",
    "            answers.append(answer)\n",
    "\n",
    "        sample_ids.append(sample_id)\n",
    "        questions.append(question)\n",
    "        img_paths.append(img_path)\n",
    "\n",
    "    return sample_ids, questions, img_paths, answers\n",
    "\n",
    "\n",
    "train_sample_ids, train_questions, train_img_paths, train_answers = get_data(root_dir=ROOT_DIR, \n",
    "                                                           split='train')\n",
    "val_sample_ids, val_questions, val_img_paths, val_answers = get_data(root_dir=ROOT_DIR, \n",
    "                                                     split='val')\n",
    "test_sample_ids, test_questions, test_img_paths, test_answers = get_data(root_dir=ROOT_DIR, \n",
    "                                                        split='test')\n",
    "answer_space = set(list(train_answers + val_answers))\n",
    "idx2label = {idx: label for idx, label in enumerate(answer_space)}\n",
    "label2idx = {label: idx for idx, label in enumerate(answer_space)}\n",
    "answer_space_len = len(answer_space)\n",
    "question_max_len = max([len(text) for text in train_questions + test_questions])\n",
    "\n",
    "train_set_size = len(train_questions)\n",
    "val_set_size = len(val_questions)\n",
    "test_set_size = len(test_questions)\n",
    "\n",
    "print(f'Number of train sample: {train_set_size}')\n",
    "print(f'Number of val sample: {val_set_size}')\n",
    "print(f'Number of test sample: {test_set_size}')\n",
    "\n",
    "print(f'Question max length: {question_max_len}')\n",
    "print(f'Answer space length: {answer_space_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-30 17:41:16 INFO  WordSegmenter:24 - Loading Word Segmentation model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import py_vncorenlp\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from contextlib import contextmanager\n",
    "\n",
    "dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "\n",
    "def text_tone_normalize(text, dict_map):\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory_change(directory):\n",
    "    original_directory = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_directory)\n",
    "\n",
    "TEXT_MODEL_ID = config['text_encoder_id']\n",
    "VNCORENLP_PATH = Path('../models/VnCoreNLP')\n",
    "ABS_VNCORENLP_PATH = VNCORENLP_PATH.resolve()\n",
    "os.makedirs(VNCORENLP_PATH, exist_ok=True)\n",
    "\n",
    "if not (ABS_VNCORENLP_PATH / 'models').exists():\n",
    "    py_vncorenlp.download_model(save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "with temporary_directory_change(ABS_VNCORENLP_PATH):\n",
    "    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], \n",
    "                                        save_dir=str(ABS_VNCORENLP_PATH))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "text_encoder_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_ID,\n",
    "                                       device_map=device)\n",
    "\n",
    "def text_processor(text):\n",
    "    text = text_tone_normalize(text, dict_map)\n",
    "    segmented_text = rdrsegmenter.word_segment(text)\n",
    "    segmented_text = ' '.join(segmented_text)\n",
    "\n",
    "    input_ids = text_encoder_tokenizer(segmented_text,\n",
    "                                       max_length=config['text_max_len'],\n",
    "                                       padding='max_length', \n",
    "                                       truncation=True,\n",
    "                                       return_token_type_ids=False,\n",
    "                                       return_tensors='pt').to(device)\n",
    "    \n",
    "    input_ids = {k: v.squeeze() for k, v in input_ids.items()}\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "# def text_processor(texts):\n",
    "#     texts = [text_tone_normalize(text, dict_map) for text in texts]\n",
    "#     segmented_texts = [' '.join(rdrsegmenter.word_segment(text)) for text in texts]\n",
    "\n",
    "#     input_ids = text_encoder_tokenizer(segmented_texts,\n",
    "#                                        max_length=config['text_max_len'],\n",
    "#                                        padding='max_length', \n",
    "#                                        truncation=True,\n",
    "#                                        return_token_type_ids=False,\n",
    "#                                        return_tensors='pt').to(device)\n",
    " \n",
    "#     return input_ids\n",
    "\n",
    "# sentence = 'Có bao nhiêu người trong bức ảnh ?' \n",
    "# phobert_outputs = text_processor(sentence)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     features = text_model(**phobert_outputs)\n",
    "#     print(features['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_MODEL_ID = config['img_encoder_id']\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "img_model = timm.create_model(\n",
    "    IMG_MODEL_ID,\n",
    "    pretrained=True,\n",
    "    num_classes=0 # remove classifier nn.Linear\n",
    ").to(device)\n",
    "data_config = timm.data.resolve_model_data_config(img_model)\n",
    "img_processor = timm.data.create_transform(**data_config, \n",
    "                                           is_training=False)\n",
    "\n",
    "# output = img_model(img_processor(image).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n",
    "# or equivalently (without needing to set num_classes=0)\n",
    "\n",
    "\n",
    "# img_model = img_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = img_model.forward_features(img_processor(image).to(device).unsqueeze(0))\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "PARAPHRASER_ID = config['paraphraser_id']\n",
    "paraphraser_tokenizer = MT5Tokenizer.from_pretrained(PARAPHRASER_ID)\n",
    "paraphraser_model = MT5ForConditionalGeneration.from_pretrained(PARAPHRASER_ID).to(device)\n",
    "\n",
    "def paraphraser(text, num_return_sequences=3):\n",
    "    inputs = paraphraser_tokenizer(text, \n",
    "                                   padding='longest', \n",
    "                                   max_length=64, \n",
    "                                   return_tensors='pt',\n",
    "                                   return_token_type_ids=False).to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    output = paraphraser_model.generate(input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            max_length=64,\n",
    "                            num_beams=5,\n",
    "                            early_stopping=True,\n",
    "                            no_repeat_ngram_size=1,\n",
    "                            num_return_sequences=num_return_sequences)\n",
    "    \n",
    "    paraphrase_lst = []\n",
    "    for beam_output in output:\n",
    "        paraphrase_lst.append(paraphraser_tokenizer.decode(beam_output, skip_special_tokens=True))\n",
    "    return paraphrase_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ViVQADataset(Dataset):\n",
    "    def __init__(self, questions, img_paths, answers, \n",
    "                 data_mode, text_processor, img_processor, \n",
    "                 paraphraser, label_encoder=None):\n",
    "        self.questions = questions\n",
    "        self.img_paths = img_paths\n",
    "        self.answers = answers\n",
    "        self.data_mode = data_mode\n",
    "        self.text_processor = text_processor\n",
    "        self.img_processor = img_processor\n",
    "        self.paraphraser = paraphraser\n",
    "        self.label_encoder = label_encoder\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        questions = self.questions[idx]\n",
    "        answers = self.answers[idx]\n",
    "        img_paths = self.img_paths[idx]\n",
    "\n",
    "        question_inputs = self.text_processor(questions)\n",
    "        #paraphrase_inputs_dict = {f'paraphrase_text_{idx+1}':  self.text_processor(text) for idx, text in enumerate(paraphrase_questions)}\n",
    "        img_pils = Image.open(img_paths).convert('RGB')\n",
    "        img_inputs = self.img_processor(img_pils).to(device)\n",
    "        label = self.label_encoder[answers]\n",
    "        \n",
    "        #img_inputs = {k: v.squeeze().to(self.device) for k, v in img_inputs.items()}\n",
    "        labels = torch.tensor(label, dtype=torch.long).to(device)\n",
    "\n",
    "        text_inputs_lst = [question_inputs]\n",
    "\n",
    "        r = random.random()\n",
    "        if self.data_mode == 'train' and config['num_paraphrase'] > 0:\n",
    "            paraphrase_questions = self.paraphraser(questions, config['num_paraphrase'])\n",
    "            paraphrase_inputs_lst = [self.text_processor(text) for text in paraphrase_questions]\n",
    "\n",
    "            if r < config['paraphrase_thresh']:\n",
    "                is_fuse_para_t = torch.ones(text_model.config.hidden_size).to(device)\n",
    "            else: \n",
    "                is_fuse_para_t = torch.zeros(text_model.config.hidden_size).to(device)\n",
    "\n",
    "            text_inputs_lst += paraphrase_inputs_lst + [is_fuse_para_t]\n",
    "        \n",
    "        data_outputs = {\n",
    "            'text_inputs_lst': text_inputs_lst,\n",
    "            'img_inputs': img_inputs,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        return data_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VAL_BATCH_SIZE = config['val_batch_size']\n",
    "    \n",
    "train_dataset = ViVQADataset(questions=train_questions,\n",
    "                             img_paths=train_img_paths,\n",
    "                             answers=train_answers,\n",
    "                             data_mode='train',\n",
    "                             text_processor=text_processor,\n",
    "                             img_processor=img_processor, \n",
    "                             paraphraser=paraphraser,\n",
    "                             label_encoder=label2idx)\n",
    "val_dataset = ViVQADataset(questions=val_questions,\n",
    "                           img_paths=val_img_paths,\n",
    "                           answers=val_answers,\n",
    "                           data_mode='val',\n",
    "                           text_processor=text_processor,\n",
    "                           img_processor=img_processor, \n",
    "                           paraphraser=paraphraser,\n",
    "                           label_encoder=label2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=VAL_BATCH_SIZE,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 27896])\n"
     ]
    }
   ],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_model, projection_dim):\n",
    "        super().__init__()\n",
    "        self.model = text_model\n",
    "        self.linear = nn.Linear(self.model.config.hidden_size, projection_dim)\n",
    "\n",
    "    def forward(self, text_inputs_lst):\n",
    "        if self.training and config['num_paraphrase'] > 0:\n",
    "            embed_lst = []\n",
    "            for text_inputs in text_inputs_lst[:-1]:\n",
    "                x = self.model(**text_inputs)\n",
    "                x = x['last_hidden_state'][:, 0, :]\n",
    "                embed_lst.append(x)\n",
    "        \n",
    "            para_features_t = torch.stack(embed_lst[1:], dim=1)\n",
    "            x = torch.sum(para_features_t, dim=1)\n",
    "            x *= text_inputs_lst[-1]\n",
    "            x = x + embed_lst[0]\n",
    "\n",
    "        else:\n",
    "            text_inputs = text_inputs_lst[0]\n",
    "            x = self.model(**text_inputs)\n",
    "            x = x['last_hidden_state'][:, 0, :]\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, img_model, projection_dim):\n",
    "        super().__init__()\n",
    "        for param in img_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.model = img_model\n",
    "        self.linear = nn.Linear(self.model.num_features * 7 * 7, projection_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.model.forward_features(inputs)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, projection_dim, hidden_dim, answer_space):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(projection_dim * 2, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim // 2, answer_space)\n",
    "\n",
    "    def forward(self, text_f, img_f):\n",
    "        x = torch.cat((img_f, text_f), 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "class OpenViVQAModel(nn.Module):\n",
    "    def __init__(self, text_encoder, img_encoder, classifier):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.img_encoder = img_encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, text_inputs, img_inputs):\n",
    "        text_f = self.text_encoder(text_inputs)\n",
    "        img_f = self.img_encoder(img_inputs)\n",
    "\n",
    "        logits = self.classifier(text_f, img_f)\n",
    "\n",
    "        return logits\n",
    "\n",
    "PROJECTION_DIM = config['projection_dim']\n",
    "HIDDEN_DIM = config['hidden_dim']\n",
    "text_encoder = TextEncoder(text_model=text_model,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ImageEncoder(img_model=img_model,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "\n",
    "model = OpenViVQAModel(text_encoder=text_encoder,\n",
    "                   img_encoder=img_encoder,\n",
    "                   classifier=classifier)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\n",
    "question = 'Có bao nhiêu con mèo trong bức ảnh?'\n",
    "batch = next(iter(train_loader))\n",
    "# batch['text_inputs_lst'][0]['input_ids'].shape\n",
    "with torch.no_grad():\n",
    "    text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "    img_inputs = batch.pop('img_inputs')\n",
    "    labels = batch.pop('labels')\n",
    "\n",
    "    logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:   0%|          | 10/3855 [00:27<2:57:04,  2.76s/batch, Batch Loss=10.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 139\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n\u001b[0;32m--> 139\u001b[0m train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst \u001b[38;5;241m=\u001b[39m train(model, \n\u001b[1;32m    140\u001b[0m                                                                  train_loader, \n\u001b[1;32m    141\u001b[0m                                                                  val_loader, \n\u001b[1;32m    142\u001b[0m                                                                  epochs\u001b[38;5;241m=\u001b[39mEPOCHS, \n\u001b[1;32m    143\u001b[0m                                                                  criterion\u001b[38;5;241m=\u001b[39mcriterion, \n\u001b[1;32m    144\u001b[0m                                                                  optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[1;32m    145\u001b[0m                                                                  patience\u001b[38;5;241m=\u001b[39mPATIENCE,\n\u001b[1;32m    146\u001b[0m                                                                  save_best_path\u001b[38;5;241m=\u001b[39mSAVE_BEST_PATH)\n",
      "Cell \u001b[0;32mIn[33], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, criterion, optimizer, patience, save_best_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m img_inputs \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(text_inputs_lst, img_inputs)\n\u001b[1;32m     90\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m     91\u001b[0m acc \u001b[38;5;241m=\u001b[39m compute_accuracy(logits, labels)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:183\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mViVQAModel.forward\u001b[0;34m(self, text_inputs, img_inputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, img_inputs):\n\u001b[0;32m---> 76\u001b[0m     text_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder(text_inputs)\n\u001b[1;32m     77\u001b[0m     img_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_encoder(img_inputs)\n\u001b[1;32m     79\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(text_f, img_f)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mTextEncoder.forward\u001b[0;34m(self, text_inputs_lst)\u001b[0m\n\u001b[1;32m      9\u001b[0m embed_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_inputs \u001b[38;5;129;01min\u001b[39;00m text_inputs_lst[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_inputs)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     13\u001b[0m     embed_lst\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/transformers/models/mbart/modeling_mbart.py:1445\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1442\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1445\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1446\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1447\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1448\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1449\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1450\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1451\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1452\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1453\u001b[0m     )\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/transformers/models/mbart/modeling_mbart.py:1077\u001b[0m, in \u001b[0;36mMBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1070\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1071\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             output_attentions,\n\u001b[1;32m   1075\u001b[0m         )\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1077\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m encoder_layer(\n\u001b[1;32m   1078\u001b[0m             hidden_states,\n\u001b[1;32m   1079\u001b[0m             attention_mask,\n\u001b[1;32m   1080\u001b[0m             layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1081\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1082\u001b[0m         )\n\u001b[1;32m   1084\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/transformers/models/mbart/modeling_mbart.py:565\u001b[0m, in \u001b[0;36mMBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    558\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[1;32m    559\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    560\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    561\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    562\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    563\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    564\u001b[0m )\n\u001b[0;32m--> 565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    566\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    568\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/thangdd_env/lib/python3.11/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def save_model(save_path, model):\n",
    "    os.makedirs('weights', exist_ok=True)\n",
    "    try:\n",
    "        state_dict = model.module.state_dict()\n",
    "    except AttributeError:\n",
    "        state_dict = model.state_dict()\n",
    "    torch.save(state_dict, save_path)\n",
    "\n",
    "def free_vram(model, optimizer):\n",
    "    del model\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "LR = config['learning_rate']\n",
    "EPOCHS = config['epochs']\n",
    "PATIENCE = config['patience']\n",
    "WEIGHT_DECAY = config['weight_decay']\n",
    "SAVE_BEST_PATH = f'./weights/{EXP_NAME}_best.pt'\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                             lr=LR,\n",
    "                             weight_decay=WEIGHT_DECAY)\n",
    "# step_size = EPOCHS * 0.4\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "#                                             step_size=step_size, \n",
    "#                                             gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    accuracy = correct / logits.size(0)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_losses = []\n",
    "    eval_accs = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            eval_losses.append(loss.item())\n",
    "            eval_accs.append(acc)\n",
    "\n",
    "    eval_loss = sum(eval_losses) / len(eval_losses)\n",
    "    eval_acc = sum(eval_accs) / len(eval_accs)\n",
    "\n",
    "    return eval_loss, eval_acc\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          train_loader, \n",
    "          val_loader, \n",
    "          epochs, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          patience=5,\n",
    "          save_best_path='./weights/best.pt'):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_loss_lst = []\n",
    "    train_acc_lst = []\n",
    "    val_loss_lst = []\n",
    "    val_acc_lst = []\n",
    "    for epoch in range(epochs):\n",
    "        train_batch_loss_lst = []\n",
    "        train_batch_acc_lst = []\n",
    "\n",
    "        epoch_iterator = tqdm(train_loader, \n",
    "                              desc=f'Epoch {epoch + 1}/{epochs}', \n",
    "                              unit='batch')\n",
    "        model.train()\n",
    "        for batch in epoch_iterator:\n",
    "            text_inputs_lst = batch.pop('text_inputs_lst')\n",
    "            img_inputs = batch.pop('img_inputs')\n",
    "            labels = batch.pop('labels')\n",
    "\n",
    "            logits = model(text_inputs_lst, img_inputs)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = compute_accuracy(logits, labels)\n",
    "\n",
    "            train_batch_loss_lst.append(loss.item())\n",
    "            train_batch_acc_lst.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_iterator.set_postfix({'Batch Loss': loss.item()})\n",
    "\n",
    "        val_loss, val_acc = evaluate(model,\n",
    "                                     val_loader,\n",
    "                                     criterion)\n",
    "\n",
    "        train_loss = sum(train_batch_loss_lst) / len(train_batch_loss_lst)\n",
    "        train_acc = sum(train_batch_acc_lst) / len(train_batch_acc_lst)\n",
    "\n",
    "        train_loss_lst.append(train_loss)\n",
    "        train_acc_lst.append(train_acc)\n",
    "        val_loss_lst.append(val_loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}: Train loss: {train_loss:.4f}\\tTrain acc: {train_acc:.4f}\\tVal loss: {val_loss:.4f}\\tVal acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_model(save_best_path, model)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping triggered after {epochs_no_improve} epochs without improvement.')\n",
    "                break\n",
    "\n",
    "    return train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst\n",
    "\n",
    "train_loss_lst, train_acc_lst, val_loss_lst, val_acc_lst = train(model, \n",
    "                                                                 train_loader, \n",
    "                                                                 val_loader, \n",
    "                                                                 epochs=EPOCHS, \n",
    "                                                                 criterion=criterion, \n",
    "                                                                 optimizer=optimizer, \n",
    "                                                                 patience=PATIENCE,\n",
    "                                                                 save_best_path=SAVE_BEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.1953\tVal acc: 0.4962\n",
      "Test loss: 2.2118\tTest acc: 0.5032\n"
     ]
    }
   ],
   "source": [
    "free_vram(model, optimizer)\n",
    "\n",
    "text_encoder = TextEncoder(text_model=text_model,\n",
    "                           projection_dim=PROJECTION_DIM)\n",
    "img_encoder = ImageEncoder(img_model=img_model,\n",
    "                         projection_dim=PROJECTION_DIM)\n",
    "classifier = Classifier(projection_dim=PROJECTION_DIM,\n",
    "                        hidden_dim=HIDDEN_DIM,\n",
    "                        answer_space=answer_space_len)\n",
    "best_model = OpenViVQAModel(text_encoder=text_encoder,\n",
    "                            img_encoder=img_encoder,\n",
    "                            classifier=classifier).to(device)\n",
    "best_model.load_state_dict(torch.load(SAVE_BEST_PATH))\n",
    "\n",
    "\n",
    "val_loss, val_acc = evaluate(model=best_model,\n",
    "                             val_loader=val_loader,\n",
    "                             criterion=criterion)\n",
    "val_loss, val_acc = round(val_loss, 4), round(val_acc, 4)\n",
    "\n",
    "\n",
    "print(f'Val loss: {val_loss}\\tVal acc: {val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(root_dir, model, text_processor, img_processor, idx2label, split):\n",
    "    if split == 'val':\n",
    "        json_path = root_dir / 'vlsp2023_dev_data.json'\n",
    "        img_dir = root_dir / 'dev-images'\n",
    "        save_path = 'public_results.json'\n",
    "    elif split == 'test':\n",
    "        json_path = root_dir / 'vlsp2023_test_data.json'\n",
    "        img_dir = root_dir / 'test-images'\n",
    "        save_path = 'private_results.json'\n",
    "        \n",
    "    with open(json_path, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "\n",
    "    images_mapping = data_dict['images']    \n",
    "    annotations = data_dict['annotations']\n",
    "\n",
    "    submit_dict = {}\n",
    "    model.eval()\n",
    "    for question_id, sample in annotations.items():\n",
    "        img_id = str(sample['image_id'])\n",
    "        question = sample['question']\n",
    "\n",
    "        img_name = images_mapping[img_id]\n",
    "        img_path = img_dir / img_name\n",
    "\n",
    "        question_inputs = text_processor(question)\n",
    "        img_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        img_inputs = img_processor(img_pil).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_inputs_lst = [question_inputs]\n",
    "            logits = model(text_inputs_lst, img_inputs)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "\n",
    "            answer = idx2label[preds]\n",
    "\n",
    "            submit_dict['question_id'] = answer\n",
    "\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(submit_dict, f, \n",
    "                  sort_keys=True, \n",
    "                  indent=4)        \n",
    "        \n",
    "evaluate(root_dir=ROOT_DIR, \n",
    "         model=best_model, \n",
    "         text_processor=text_processor, \n",
    "         img_processor=img_processor, \n",
    "         idx2label=idx2label, \n",
    "         split='val')\n",
    "\n",
    "evaluate(root_dir=ROOT_DIR, \n",
    "         model=best_model, \n",
    "         text_processor=text_processor, \n",
    "         img_processor=img_processor, \n",
    "         idx2label=idx2label, \n",
    "         split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_table = wandb.Table(\n",
    "    columns=list(config.keys())+['val_loss', 'val_acc', 'test_loss', 'test_acc'], \n",
    "    data=[list(config.values()) + [val_loss, val_acc, test_loss, test_acc]]\n",
    ")\n",
    "wandb.log({\"Exp_table\": exp_table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
